{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Couldn't instantiate the backend tokenizer from one of: \n(1) a `tokenizers` library serialization file, \n(2) a slow tokenizer instance to convert or \n(3) an equivalent slow tokenizer class to instantiate and convert. \nYou need to have sentencepiece installed to convert a slow tokenizer to a fast one.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/willy/comp5214-groundedness-kgd/QG_T0_QuAC.ipynb Cell 2'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Beez115.ece.ust.hk/home/willy/comp5214-groundedness-kgd/QG_T0_QuAC.ipynb#ch0000001vscode-remote?line=0'>1</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mbigscience/T0_3B\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Beez115.ece.ust.hk/home/willy/comp5214-groundedness-kgd/QG_T0_QuAC.ipynb#ch0000001vscode-remote?line=1'>2</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModelForSeq2SeqLM\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mbigscience/T0_3B\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/KGD/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py:546\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/willy/miniconda3/envs/KGD/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py?line=543'>544</a>\u001b[0m tokenizer_class_py, tokenizer_class_fast \u001b[39m=\u001b[39m TOKENIZER_MAPPING[\u001b[39mtype\u001b[39m(config)]\n\u001b[1;32m    <a href='file:///home/willy/miniconda3/envs/KGD/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py?line=544'>545</a>\u001b[0m \u001b[39mif\u001b[39;00m tokenizer_class_fast \u001b[39mand\u001b[39;00m (use_fast \u001b[39mor\u001b[39;00m tokenizer_class_py \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> <a href='file:///home/willy/miniconda3/envs/KGD/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py?line=545'>546</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer_class_fast\u001b[39m.\u001b[39;49mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/willy/miniconda3/envs/KGD/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py?line=546'>547</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/willy/miniconda3/envs/KGD/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py?line=547'>548</a>\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer_class_py \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/KGD/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1780\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/willy/miniconda3/envs/KGD/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=1776'>1777</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   <a href='file:///home/willy/miniconda3/envs/KGD/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=1777'>1778</a>\u001b[0m         logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mloading file \u001b[39m\u001b[39m{\u001b[39;00mfile_path\u001b[39m}\u001b[39;00m\u001b[39m from cache at \u001b[39m\u001b[39m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> <a href='file:///home/willy/miniconda3/envs/KGD/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=1779'>1780</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_from_pretrained(\n\u001b[1;32m   <a href='file:///home/willy/miniconda3/envs/KGD/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=1780'>1781</a>\u001b[0m     resolved_vocab_files,\n\u001b[1;32m   <a href='file:///home/willy/miniconda3/envs/KGD/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=1781'>1782</a>\u001b[0m     pretrained_model_name_or_path,\n\u001b[1;32m   <a href='file:///home/willy/miniconda3/envs/KGD/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=1782'>1783</a>\u001b[0m     init_configuration,\n\u001b[1;32m   <a href='file:///home/willy/miniconda3/envs/KGD/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=1783'>1784</a>\u001b[0m     \u001b[39m*\u001b[39;49minit_inputs,\n\u001b[1;32m   <a href='file:///home/willy/miniconda3/envs/KGD/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=1784'>1785</a>\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   <a href='file:///home/willy/miniconda3/envs/KGD/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=1785'>1786</a>\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   <a href='file:///home/willy/miniconda3/envs/KGD/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=1786'>1787</a>\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   <a href='file:///home/willy/miniconda3/envs/KGD/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=1787'>1788</a>\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/KGD/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1915\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/willy/miniconda3/envs/KGD/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=1912'>1913</a>\u001b[0m \u001b[39m# Instantiate tokenizer.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/willy/miniconda3/envs/KGD/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=1913'>1914</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///home/willy/miniconda3/envs/KGD/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=1914'>1915</a>\u001b[0m     tokenizer \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(\u001b[39m*\u001b[39;49minit_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minit_kwargs)\n\u001b[1;32m   <a href='file:///home/willy/miniconda3/envs/KGD/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=1915'>1916</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[1;32m   <a href='file:///home/willy/miniconda3/envs/KGD/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=1916'>1917</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[1;32m   <a href='file:///home/willy/miniconda3/envs/KGD/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=1917'>1918</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUnable to load vocabulary from file. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///home/willy/miniconda3/envs/KGD/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=1918'>1919</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///home/willy/miniconda3/envs/KGD/lib/python3.8/site-packages/transformers/tokenization_utils_base.py?line=1919'>1920</a>\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/KGD/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py:130\u001b[0m, in \u001b[0;36mT5TokenizerFast.__init__\u001b[0;34m(self, vocab_file, tokenizer_file, eos_token, unk_token, pad_token, extra_ids, additional_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/willy/miniconda3/envs/KGD/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py?line=123'>124</a>\u001b[0m     \u001b[39mif\u001b[39;00m extra_tokens \u001b[39m!=\u001b[39m extra_ids:\n\u001b[1;32m    <a href='file:///home/willy/miniconda3/envs/KGD/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py?line=124'>125</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///home/willy/miniconda3/envs/KGD/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py?line=125'>126</a>\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBoth extra_ids (\u001b[39m\u001b[39m{\u001b[39;00mextra_ids\u001b[39m}\u001b[39;00m\u001b[39m) and additional_special_tokens (\u001b[39m\u001b[39m{\u001b[39;00madditional_special_tokens\u001b[39m}\u001b[39;00m\u001b[39m) are provided to T5Tokenizer. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/willy/miniconda3/envs/KGD/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py?line=126'>127</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mIn this case the additional_special_tokens must include the extra_ids tokens\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/willy/miniconda3/envs/KGD/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py?line=127'>128</a>\u001b[0m         )\n\u001b[0;32m--> <a href='file:///home/willy/miniconda3/envs/KGD/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py?line=129'>130</a>\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    <a href='file:///home/willy/miniconda3/envs/KGD/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py?line=130'>131</a>\u001b[0m     vocab_file,\n\u001b[1;32m    <a href='file:///home/willy/miniconda3/envs/KGD/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py?line=131'>132</a>\u001b[0m     tokenizer_file\u001b[39m=\u001b[39;49mtokenizer_file,\n\u001b[1;32m    <a href='file:///home/willy/miniconda3/envs/KGD/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py?line=132'>133</a>\u001b[0m     eos_token\u001b[39m=\u001b[39;49meos_token,\n\u001b[1;32m    <a href='file:///home/willy/miniconda3/envs/KGD/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py?line=133'>134</a>\u001b[0m     unk_token\u001b[39m=\u001b[39;49munk_token,\n\u001b[1;32m    <a href='file:///home/willy/miniconda3/envs/KGD/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py?line=134'>135</a>\u001b[0m     pad_token\u001b[39m=\u001b[39;49mpad_token,\n\u001b[1;32m    <a href='file:///home/willy/miniconda3/envs/KGD/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py?line=135'>136</a>\u001b[0m     extra_ids\u001b[39m=\u001b[39;49mextra_ids,\n\u001b[1;32m    <a href='file:///home/willy/miniconda3/envs/KGD/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py?line=136'>137</a>\u001b[0m     additional_special_tokens\u001b[39m=\u001b[39;49madditional_special_tokens,\n\u001b[1;32m    <a href='file:///home/willy/miniconda3/envs/KGD/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py?line=137'>138</a>\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    <a href='file:///home/willy/miniconda3/envs/KGD/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py?line=138'>139</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///home/willy/miniconda3/envs/KGD/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py?line=140'>141</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_file \u001b[39m=\u001b[39m vocab_file\n\u001b[1;32m    <a href='file:///home/willy/miniconda3/envs/KGD/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py?line=141'>142</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcan_save_slow_tokenizer \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_file \u001b[39melse\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/KGD/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py:118\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/willy/miniconda3/envs/KGD/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=115'>116</a>\u001b[0m     fast_tokenizer \u001b[39m=\u001b[39m convert_slow_tokenizer(slow_tokenizer)\n\u001b[1;32m    <a href='file:///home/willy/miniconda3/envs/KGD/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=116'>117</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/willy/miniconda3/envs/KGD/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=117'>118</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///home/willy/miniconda3/envs/KGD/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=118'>119</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt instantiate the backend tokenizer from one of: \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/willy/miniconda3/envs/KGD/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=119'>120</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(1) a `tokenizers` library serialization file, \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/willy/miniconda3/envs/KGD/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=120'>121</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(2) a slow tokenizer instance to convert or \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/willy/miniconda3/envs/KGD/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=121'>122</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(3) an equivalent slow tokenizer class to instantiate and convert. \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/willy/miniconda3/envs/KGD/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=122'>123</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou need to have sentencepiece installed to convert a slow tokenizer to a fast one.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/willy/miniconda3/envs/KGD/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=123'>124</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/willy/miniconda3/envs/KGD/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=125'>126</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tokenizer \u001b[39m=\u001b[39m fast_tokenizer\n\u001b[1;32m    <a href='file:///home/willy/miniconda3/envs/KGD/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py?line=127'>128</a>\u001b[0m \u001b[39mif\u001b[39;00m slow_tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Couldn't instantiate the backend tokenizer from one of: \n(1) a `tokenizers` library serialization file, \n(2) a slow tokenizer instance to convert or \n(3) an equivalent slow tokenizer class to instantiate and convert. \nYou need to have sentencepiece installed to convert a slow tokenizer to a fast one."
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/T0_3B\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"bigscience/T0_3B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "321b428eb2f22c8dd426e302012007662aaddde80543b37e06bd4b8428c821cf"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('KGD')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
